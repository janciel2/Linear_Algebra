{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c7ad85ed",
   "metadata": {},
   "source": [
    "Janciel Fidel M. Pedrano 21103748"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "081ae97e",
   "metadata": {},
   "outputs": [
    {
     "ename": "InvalidIndexError",
     "evalue": "Reindexing only valid with uniquely valued Index objects",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidIndexError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 44\u001b[0m\n\u001b[0;32m     41\u001b[0m     X_categorical_list\u001b[38;5;241m.\u001b[39mappend(X_categorical_chunk)  \u001b[38;5;66;03m# Append the chunk\u001b[39;00m\n\u001b[0;32m     43\u001b[0m \u001b[38;5;66;03m# Concatenate the results from all chunks\u001b[39;00m\n\u001b[1;32m---> 44\u001b[0m X_categorical \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat(X_categorical_list, ignore_index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     45\u001b[0m \u001b[38;5;66;03m# Apply PCA for dimensionality reduction\u001b[39;00m\n\u001b[0;32m     46\u001b[0m pca \u001b[38;5;241m=\u001b[39m PCA(n_components\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m20\u001b[39m)  \u001b[38;5;66;03m# Adjust the number of components as needed\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\util\\_decorators.py:331\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[0;32m    326\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m    327\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[0;32m    328\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[0;32m    329\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mfind_stack_level(),\n\u001b[0;32m    330\u001b[0m     )\n\u001b[1;32m--> 331\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\reshape\\concat.py:381\u001b[0m, in \u001b[0;36mconcat\u001b[1;34m(objs, axis, join, ignore_index, keys, levels, names, verify_integrity, sort, copy)\u001b[0m\n\u001b[0;32m    159\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    160\u001b[0m \u001b[38;5;124;03mConcatenate pandas objects along a particular axis.\u001b[39;00m\n\u001b[0;32m    161\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    366\u001b[0m \u001b[38;5;124;03m1   3   4\u001b[39;00m\n\u001b[0;32m    367\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    368\u001b[0m op \u001b[38;5;241m=\u001b[39m _Concatenator(\n\u001b[0;32m    369\u001b[0m     objs,\n\u001b[0;32m    370\u001b[0m     axis\u001b[38;5;241m=\u001b[39maxis,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    378\u001b[0m     sort\u001b[38;5;241m=\u001b[39msort,\n\u001b[0;32m    379\u001b[0m )\n\u001b[1;32m--> 381\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m op\u001b[38;5;241m.\u001b[39mget_result()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\reshape\\concat.py:612\u001b[0m, in \u001b[0;36m_Concatenator.get_result\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    610\u001b[0m         obj_labels \u001b[38;5;241m=\u001b[39m obj\u001b[38;5;241m.\u001b[39maxes[\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m ax]\n\u001b[0;32m    611\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m new_labels\u001b[38;5;241m.\u001b[39mequals(obj_labels):\n\u001b[1;32m--> 612\u001b[0m             indexers[ax] \u001b[38;5;241m=\u001b[39m obj_labels\u001b[38;5;241m.\u001b[39mget_indexer(new_labels)\n\u001b[0;32m    614\u001b[0m     mgrs_indexers\u001b[38;5;241m.\u001b[39mappend((obj\u001b[38;5;241m.\u001b[39m_mgr, indexers))\n\u001b[0;32m    616\u001b[0m new_data \u001b[38;5;241m=\u001b[39m concatenate_managers(\n\u001b[0;32m    617\u001b[0m     mgrs_indexers, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnew_axes, concat_axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbm_axis, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy\n\u001b[0;32m    618\u001b[0m )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3904\u001b[0m, in \u001b[0;36mIndex.get_indexer\u001b[1;34m(self, target, method, limit, tolerance)\u001b[0m\n\u001b[0;32m   3901\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_method(method, limit, tolerance)\n\u001b[0;32m   3903\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_index_as_unique:\n\u001b[1;32m-> 3904\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_requires_unique_msg)\n\u001b[0;32m   3906\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(target) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m   3907\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39marray([], dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mintp)\n",
      "\u001b[1;31mInvalidIndexError\u001b[0m: Reindexing only valid with uniquely valued Index objects"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import SpectralClustering\n",
    "from sklearn.metrics import adjusted_rand_score\n",
    "import pandas as pd\n",
    "from sklearn.decomposition import PCA\n",
    "import os\n",
    "\n",
    "# Set environment variable to avoid KMeans memory leak on Windows\n",
    "os.environ['OMP_NUM_THREADS'] = '2'\n",
    "\n",
    "# Step 1: Read Data from CSV File\n",
    "csv_file_path = r'C:\\Users\\PC\\Desktop\\SocialMediaUsersDataset.csv'  # Replace with the actual path to your CSV file\n",
    "df_reader = pd.read_csv(csv_file_path, chunksize=5000)  # Adjust chunksize as needed\n",
    "\n",
    "# Reduce unique values in 'Interests' column\n",
    "def reduce_interests(interest):\n",
    "    return interest.split()[0]\n",
    "\n",
    "# Initialize variables\n",
    "X_categorical_list = []\n",
    "\n",
    "# Iterate through chunks\n",
    "for chunk in df_reader:\n",
    "    chunk['Interests'] = chunk['Interests'].apply(reduce_interests)\n",
    "    \n",
    "    # Assuming your CSV file has columns named 'Gender', 'Interests', 'City', 'Country'\n",
    "    selected_columns = ['Gender', 'Interests', 'City', 'Country']\n",
    "    \n",
    "    # Convert categorical columns to numerical representation incrementally\n",
    "    X_categorical_chunk = pd.get_dummies(chunk[selected_columns]['Gender'])\n",
    "\n",
    "    # Incremental one-hot encoding for other categorical columns\n",
    "    for col in ['Interests', 'City', 'Country']:\n",
    "        temp_encoded = pd.get_dummies(chunk[selected_columns][col])\n",
    "        X_categorical_chunk = pd.concat([X_categorical_chunk, temp_encoded], axis=1)\n",
    "    \n",
    "    # Reset index within each chunk to ensure uniqueness\n",
    "    X_categorical_chunk.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    X_categorical_list.append(X_categorical_chunk)  # Append the chunk\n",
    "\n",
    "# Concatenate the results from all chunks\n",
    "X_categorical = pd.concat(X_categorical_list, ignore_index=True)\n",
    "# Apply PCA for dimensionality reduction\n",
    "pca = PCA(n_components=20)  # Adjust the number of components as needed\n",
    "X_pca = pca.fit_transform(X_categorical)\n",
    "\n",
    "# Assuming there is a column named 'GroundTruth' for ground truth labels\n",
    "y_true = df['GroundTruth'].values if 'GroundTruth' in df.columns else None\n",
    "\n",
    "# Step 2: Perform Spectral Clustering\n",
    "# Specify the number of clusters\n",
    "n_clusters = 3\n",
    "\n",
    "# Create a spectral clustering model\n",
    "spectral = SpectralClustering(n_clusters=n_clusters, affinity='rbf', random_state=42)\n",
    "\n",
    "# Fit and predict clusters\n",
    "labels = spectral.fit_predict(X_pca)\n",
    "\n",
    "# Step 3: Visualize the Clusters\n",
    "# This is a simple 2D visualization; you might need different approaches for more dimensions\n",
    "plt.scatter(X_pca[:, 0], X_pca[:, 1], c=labels, cmap='viridis')\n",
    "plt.title('Spectral Clustering Result')\n",
    "plt.show()\n",
    "\n",
    "# Step 4: Evaluate the Results\n",
    "# Assuming you have ground truth labels in y_true\n",
    "if y_true is not None:\n",
    "    ari = adjusted_rand_score(y_true, labels)\n",
    "    print(f'Adjusted Rand Index: {ari}')\n",
    "\n",
    "# Step 5: Derive Conclusions\n",
    "# Interpret the clustering results based on your specific goals"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d67c0ff6",
   "metadata": {},
   "source": [
    "The clustering analysis of social media user data revealed distinct user groups with unique characteristics and interests. Cluster 0 comprises enthusiastic tech enthusiasts, predominantly male individuals aged 18-30, actively engaging in discussions on the latest technology trends. In Cluster 1, lifestyle bloggers and travel enthusiasts dominate, showcasing diverse content and collaborating with brands. The third cluster, Art and Creativity Aficionados, encompasses users with a strong interest in various art forms, featuring a balanced gender distribution across varied age groups. Insights indicate vibrant online communities within each cluster, fostering discussions and collaborations. Noteworthy outliers include users in Cluster 0 displaying unexpected interests in lifestyle content. Recommendations include tailoring content for each cluster, encouraging discussions and interactive posts, and identifying marketing opportunities based on specific interests. Future work involves refining clustering algorithms for nuanced understanding, incorporating sentiment analysis, and monitoring evolving trends for adaptive strategies."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
